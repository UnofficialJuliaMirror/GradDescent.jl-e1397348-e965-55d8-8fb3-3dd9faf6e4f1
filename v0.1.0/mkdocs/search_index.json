{
    "docs": [
        {
            "location": "/", 
            "text": "GradDescent\n\n\n#\n\n\nGradDescent\n \n \nModule\n.\n\n\nGradient Descent optimizers for Julia.\n\n\nIntroduction\n\n\nThis package abstracts the \"boilerplate\" code necessary for gradient descent. Gradient descent is \"a way to minimize an objective function $J(\u03b8)$ parameterized by a model's parameters $\u03b8 \u2208 R\u1d48$\" (Ruder 2017). Gradient descent finds $\u03b8$ which minizes $J$ by iterating over the following update\n\n\n$\u03b8 = \u03b8 - \u03b7 \u2207J(\u03b8)$\n\n\nuntil convergence of $\u03b8$. Certainly, the gradient calculation is model specific, however the learning rate $\u03b7$ (at a given iteration) is not. Instead there are many different gradient descent variants which determine the learning rate. Each type of gradient descent optimizer has its own pros/cons. For most of these optimizers, the calculation of the learning rate is based on the value of the gradient (evaluated at a particular $\u03b8$) and a few (unrelated to the model) hyperparameters. \n\n\nThe purpose of this package is to allow the user to focus on the calculation of the gradients and not worry about the code for the gradient descent optimizer. I envision a user implementing his/her gradients, experimenting with various optimizers, and modifying the gradients as necessary.\n\n\nExamples\n\n\nQuadratic Function\n\n\nHere I demonstrate a very simple example - minimizing $x\u00b2$. In this example, I use \"Adagrad\", a common gradient descent optimizer.\n\n\nusing GradDescent\n\n# objective function and gradient of objective function\nJ(x) = x ^ 2\ndJ(x) = 2 * x\n\n# number of epochs\nepochs = 1000\n\n# instantiation of Adagrad optimizer with learning rate of 1.0\n# note that this learning rate is likely to high for a\n# high dimensional case\nopt = Adagrad(\u03b7=1.0)\n\n# initial value for x (usually initialized with a random value)\nx = 20.0\n\nfor i in 1:epochs\n    # calculate the gradient wrt to the current x\n    g = dJ(x)\n\n    # change to the current x\n    \u03b4 = update(opt, g)\n    x -= \u03b4\nend\n\n\n\n\nLinear Regression\n\n\nNext I demonstrate a more common example - determining the coefficients of a linear model. Here I use \"Adam\" an extension of \"Adagrad\". In this example, we minimize the mean squared error of the predicted outcome and the actual outcome. The parameter space is the coefficients of the regression model.\n\n\nusing GradDescent, Distributions, ReverseDiff\n\nsrand(1) # set seed\nn = 1000 # number of observations\nd = 10   # number of covariates\nX = rand(Normal(), n, d) # simulated covariates\nb = rand(Normal(), d)    # generated coefficients\n\u03f5 = rand(Normal(0.0, 0.1), n) # noise\nY = X * b + \u03f5 # observed outcome\nobj(Y, X, b) = mean((Y - X * b) .^ 2) # objective to minimize\n\nepochs = 100 # number of epochs\n\n\u03b8 = rand(Normal(), d) # initialize model parameters\nopt = Adam(\u03b1=1.0)  # initalize optimizer with learning rate 1.0\n\nfor i in 1:epochs\n    # here we use automatic differentiation to calculate \n    # the gradient at a value\n    # an analytically derived gradient is not required\n    g = ReverseDiff.gradient(\u03b8 -\n obj(Y, X, \u03b8), \u03b8)\n\n    \u03b4 = update(opt, g)\n    \u03b8 -= \u03b4\nend\n\n\n\n\nVariational Inference\n\n\nFinally, I end with an example of black box variational inference (which is what initially motivated this package). Variational inference is a framework for approximating Bayesian posterior distributions using optimization. Most recent algorithms involve monte carlo estimation of gradients in conjuction with gradient ascent. Using \nGradDescent\n, we can focus on the gradient calculation without worrying too much about tracking learning rate parameters.\n\n\nIn this example we perform a full bayesian analysis on a simple model - normally distribution data with known variance. We place a \"noninformative\" Normal prior on the mean.\n\n\nusing Distributions, ForwardDiff, GradDescent, StatsFuns\n\nsrand(1) # set seed\nn = 1000 # number of observations\n\u03bc_true = 3.0  # true mean\n\u03c3_true = 1.0 # true standard deviation\nx = rand(Normal(\u03bc_true, \u03c3_true), n) # simulate data\n\n# prior on mean\nprior = Normal(0.0, 100.0)\n\n# initialize variational parameters\n\u03bb = rand(Normal(), 1, 2)\n\u03bb[2] = softplus(\u03bb[2])\n\n# initialize optimizer\nopt = Adam(\u03b1=1.0)\n\nS = 10 # number of monte carlo simulations for gradient estimation\nepochs = 50 # number of epochs\n\nfor i in 1:epochs\n    # draw S samples from q\n    z = rand(Normal(\u03bb[1], softplus(\u03bb[2])), S)\n\n    # joint density calculations\n    log_prior = logpdf(prior, z)\n    log_lik = map(zi -\n loglikelihood(Normal(zi, \u03c3_true), x), z)\n    joint = log_prior + log_lik\n\n    # log variational densities\n    entropy = logpdf(Normal(\u03bb[1], softplus(\u03bb[2])), z)\n\n    # score function calculations\n    qg = ForwardDiff.jacobian(x -\n logpdf(Normal(x[1], x[2]), \n                                          z), \n                              [\u03bb[1], softplus(\u03bb[2])])\n\n    # construct monte carlo samples st the expected value is the gradient\n    # of the ELBO\n    f = qg .* (joint - entropy)\n    h = qg\n    a = sum(diag(cov(f, h))) / sum(diag(cov(h)))\n    g = mean(f - a * h, 1) # compute gradient\n\n    # perform gradient ascent step\n    \u03b4 = update(opt, g)\n    \u03bb += \u03b4\n\n    # truncate variational standard deviation\n    # don't allow it to be too close to 0.0\n    \u03bb[2] = \u03bb[2] \n invsoftplus(1e-5) ? invsoftplus(1e-5) : \u03bb[2]\nend\n\n# after gradient ascent, apply softplus function\n\u03bb[2] = softplus(\u03bb[2])\n\n\n\n\nsource\n\n\n\n\nIndex", 
            "title": "Home"
        }, 
        {
            "location": "/#graddescent", 
            "text": "#  GradDescent     Module .  Gradient Descent optimizers for Julia.  Introduction  This package abstracts the \"boilerplate\" code necessary for gradient descent. Gradient descent is \"a way to minimize an objective function $J(\u03b8)$ parameterized by a model's parameters $\u03b8 \u2208 R\u1d48$\" (Ruder 2017). Gradient descent finds $\u03b8$ which minizes $J$ by iterating over the following update  $\u03b8 = \u03b8 - \u03b7 \u2207J(\u03b8)$  until convergence of $\u03b8$. Certainly, the gradient calculation is model specific, however the learning rate $\u03b7$ (at a given iteration) is not. Instead there are many different gradient descent variants which determine the learning rate. Each type of gradient descent optimizer has its own pros/cons. For most of these optimizers, the calculation of the learning rate is based on the value of the gradient (evaluated at a particular $\u03b8$) and a few (unrelated to the model) hyperparameters.   The purpose of this package is to allow the user to focus on the calculation of the gradients and not worry about the code for the gradient descent optimizer. I envision a user implementing his/her gradients, experimenting with various optimizers, and modifying the gradients as necessary.  Examples  Quadratic Function  Here I demonstrate a very simple example - minimizing $x\u00b2$. In this example, I use \"Adagrad\", a common gradient descent optimizer.  using GradDescent\n\n# objective function and gradient of objective function\nJ(x) = x ^ 2\ndJ(x) = 2 * x\n\n# number of epochs\nepochs = 1000\n\n# instantiation of Adagrad optimizer with learning rate of 1.0\n# note that this learning rate is likely to high for a\n# high dimensional case\nopt = Adagrad(\u03b7=1.0)\n\n# initial value for x (usually initialized with a random value)\nx = 20.0\n\nfor i in 1:epochs\n    # calculate the gradient wrt to the current x\n    g = dJ(x)\n\n    # change to the current x\n    \u03b4 = update(opt, g)\n    x -= \u03b4\nend  Linear Regression  Next I demonstrate a more common example - determining the coefficients of a linear model. Here I use \"Adam\" an extension of \"Adagrad\". In this example, we minimize the mean squared error of the predicted outcome and the actual outcome. The parameter space is the coefficients of the regression model.  using GradDescent, Distributions, ReverseDiff\n\nsrand(1) # set seed\nn = 1000 # number of observations\nd = 10   # number of covariates\nX = rand(Normal(), n, d) # simulated covariates\nb = rand(Normal(), d)    # generated coefficients\n\u03f5 = rand(Normal(0.0, 0.1), n) # noise\nY = X * b + \u03f5 # observed outcome\nobj(Y, X, b) = mean((Y - X * b) .^ 2) # objective to minimize\n\nepochs = 100 # number of epochs\n\n\u03b8 = rand(Normal(), d) # initialize model parameters\nopt = Adam(\u03b1=1.0)  # initalize optimizer with learning rate 1.0\n\nfor i in 1:epochs\n    # here we use automatic differentiation to calculate \n    # the gradient at a value\n    # an analytically derived gradient is not required\n    g = ReverseDiff.gradient(\u03b8 -  obj(Y, X, \u03b8), \u03b8)\n\n    \u03b4 = update(opt, g)\n    \u03b8 -= \u03b4\nend  Variational Inference  Finally, I end with an example of black box variational inference (which is what initially motivated this package). Variational inference is a framework for approximating Bayesian posterior distributions using optimization. Most recent algorithms involve monte carlo estimation of gradients in conjuction with gradient ascent. Using  GradDescent , we can focus on the gradient calculation without worrying too much about tracking learning rate parameters.  In this example we perform a full bayesian analysis on a simple model - normally distribution data with known variance. We place a \"noninformative\" Normal prior on the mean.  using Distributions, ForwardDiff, GradDescent, StatsFuns\n\nsrand(1) # set seed\nn = 1000 # number of observations\n\u03bc_true = 3.0  # true mean\n\u03c3_true = 1.0 # true standard deviation\nx = rand(Normal(\u03bc_true, \u03c3_true), n) # simulate data\n\n# prior on mean\nprior = Normal(0.0, 100.0)\n\n# initialize variational parameters\n\u03bb = rand(Normal(), 1, 2)\n\u03bb[2] = softplus(\u03bb[2])\n\n# initialize optimizer\nopt = Adam(\u03b1=1.0)\n\nS = 10 # number of monte carlo simulations for gradient estimation\nepochs = 50 # number of epochs\n\nfor i in 1:epochs\n    # draw S samples from q\n    z = rand(Normal(\u03bb[1], softplus(\u03bb[2])), S)\n\n    # joint density calculations\n    log_prior = logpdf(prior, z)\n    log_lik = map(zi -  loglikelihood(Normal(zi, \u03c3_true), x), z)\n    joint = log_prior + log_lik\n\n    # log variational densities\n    entropy = logpdf(Normal(\u03bb[1], softplus(\u03bb[2])), z)\n\n    # score function calculations\n    qg = ForwardDiff.jacobian(x -  logpdf(Normal(x[1], x[2]), \n                                          z), \n                              [\u03bb[1], softplus(\u03bb[2])])\n\n    # construct monte carlo samples st the expected value is the gradient\n    # of the ELBO\n    f = qg .* (joint - entropy)\n    h = qg\n    a = sum(diag(cov(f, h))) / sum(diag(cov(h)))\n    g = mean(f - a * h, 1) # compute gradient\n\n    # perform gradient ascent step\n    \u03b4 = update(opt, g)\n    \u03bb += \u03b4\n\n    # truncate variational standard deviation\n    # don't allow it to be too close to 0.0\n    \u03bb[2] = \u03bb[2]   invsoftplus(1e-5) ? invsoftplus(1e-5) : \u03bb[2]\nend\n\n# after gradient ascent, apply softplus function\n\u03bb[2] = softplus(\u03bb[2])  source", 
            "title": "GradDescent"
        }, 
        {
            "location": "/#index", 
            "text": "", 
            "title": "Index"
        }
    ]
}